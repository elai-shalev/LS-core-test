version: '2'
image_name: ollama-llama-stack-config
apis:
  - agents
  - inference
  - safety
  - telemetry
  - tool_runtime
  - vector_io

providers:
  vector_io:
    - config:
        kvstore:
          db_path: /tmp/faiss_store.db
          type: sqlite
      provider_id: faiss
      provider_type: inline::faiss

  agents:
  - config:
      persistence_store:
        db_path: /tmp/agents_store.db
        namespace: null
        type: sqlite
      responses_store:
        db_path: /tmp/responses_store.db
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference


  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL:=http://localhost:8000/v1}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        api_token: ${env.VLLM_API_TOKEN:=fake}
        tls_verify: ${env.VLLM_TLS_VERIFY:=false}

  tool_runtime:
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}
      module: null

  telemetry:
    - config:
        service_name: 'llama-stack'
        sinks: console,sqlite
        sqlite_db_path: /tmp/trace_store.db
      provider_id: meta-reference
      provider_type: inline::meta-reference

metadata_store:
  type: sqlite
  db_path: /tmp/registry.db
  namespace: null

inference_store:
  type: sqlite
  db_path: /tmp/inference_store.db
