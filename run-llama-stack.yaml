version: '2'
image_name: ollama-llama-stack-config
apis:
  - inference
  - safety
  - tool_runtime
  - telemetry

providers:
  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL:=http://localhost:8000/v1}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        api_token: ${env.VLLM_API_TOKEN:=fake}
        tls_verify: ${env.VLLM_TLS_VERIFY:=false}
  tool_runtime:
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}

  telemetry:
    - config:
        service_name: 'llama-stack'
        sinks: console,sqlite
        sqlite_db_path: /tmp/trace_store.db
      provider_id: meta-reference
      provider_type: inline::meta-reference

metadata_store:
  type: sqlite
  db_path: /tmp/registry.db
  namespace: null

inference_store:
  type: sqlite
  db_path: /tmp/inference_store.db
