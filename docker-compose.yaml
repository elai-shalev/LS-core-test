services:
  llama-stack:
    image: docker.io/llamastack/distribution-starter
    container_name: llama-stack
    ports:
      - "8321:8321"
    volumes:
      - ./run-llama-stack.yaml:/app-root/run.yaml:Z
    # entrypoint: ["sleep", "10000h"]
    entrypoint: ["python", "-m", "llama_stack.core.server.server"]
    command: ["/app-root/run.yaml", "--port", "8321"]
    environment:
      - LLAMA_STACK_LOG=DEBUG
      - VLLM_URL=${VLLM_URL}
      - VLLM_API_TOKEN=test
      - VLLM_MAX_TOKENS=4096
      - VLLM_TLS_VERIFY=false
      - INFERENCE_MODEL=gemma3:27b-it-qat
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8321/v1/health"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s
    networks:
      - lightspeednet

  lightspeed-stack:
    image: quay.io/lightspeed-core/lightspeed-stack:latest
    container_name: lightspeed-stack
    ports:
      - "8080:8080"
    volumes:
      - ./config.yaml:/app-root/lightspeed-stack.yaml:Z
    depends_on:
      llama-stack:
        condition: service_healthy
    networks:
      - lightspeednet

networks:
  lightspeednet:
    driver: bridge
